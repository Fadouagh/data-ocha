{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning parameters\n",
    "\n",
    "Author: Fadoua Ghourabi (fadouaghourabi@gmail.com)\n",
    "\n",
    "Date: June 20, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import uniform\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "\n",
    "Recall cross validation from the session _linear models_. Instead of splitting the data into one train set and one test set, cross validation splits data repeatedly and multiple models are trained. The most common cross validation is K-fold cross validation. When performing K-fold, the model is trained and tested on K partitions of the dats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Perform 5-fold cross validation, i.e. cv = 5\n",
    "scores = cross_val_score(model, iris.data, iris.target, cv=5, scoring='accuracy')\n",
    "print(\"Cross validation score: {}\".format(scores))\n",
    "print(\"Cross validation mean score: {}\".format(scores.mean()))\n",
    "print(\"Cross validation std score: {}\".format(scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``cross_val_score`` computes the scores of the test sets. Since it is a classification problem, the score function is the mean accuracy on the given test set.\n",
    "$$\\text{accuracy} = \\frac{\\text{# correct predictions}}{\\text{# of target values}}$$\n",
    "\n",
    "**Ohno san asks \"How do I know whether the model is overfitting or underfitting?\".** In other words, how do I get the score of the training set so that we can compare with the score of the test set? Unfortunately, ``cross_val_score`` does not give such information, but we can get it manually as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fitting_KFold(model, X, y, kfold):\n",
    "    train_scores, test_scores = [], []\n",
    "    cv = KFold(kfold) # split into k partitions of train and test sets\n",
    "    \n",
    "    # for each partition, we compute the train score and test score\n",
    "    for train, test in cv.split(X):\n",
    "        model.fit(X[train], y[train])\n",
    "        train_scores.append(model.score(X[train], y[train]))\n",
    "        test_scores.append(model.score(X[test], y[test]))\n",
    "\n",
    "    mean_train_score = np.mean(train_scores)\n",
    "    mean_test_score = np.mean(test_scores)\n",
    "    \n",
    "    return mean_train_score, mean_test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_fitting_KFold(LogisticRegression(), iris.data, iris.target, 4) # overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_fitting_KFold(GradientBoostingRegressor(), boston.data, boston.target, 3) # overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_fitting_KFold(Ridge(), diabetes.data, diabetes.target, 3) # very bad model: overfitting & underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search with cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search is an approach to parameter tuning that will evaluate a model for each combination of algorithm parameters specified in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = datasets.load_diabetes()\n",
    "dataset = datasets.load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, random_state=0)\n",
    "\n",
    "# a range of alpha values to test\n",
    "alphas = np.array([0.1,0.5,1,10,100])\n",
    "\n",
    "# a range of solvers\n",
    "solver = ['sag', 'saga','svd','sparse_cg','lsqr','cholesky']\n",
    "param_grid = {'alpha': alphas, 'solver': solver}\n",
    "\n",
    "# create and fit a ridge regression model\n",
    "model = Ridge()\n",
    "\n",
    "# test the model on diffrent alphas and solvers\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='r2', cv=3)\n",
    "grid.fit(dataset.data, dataset.target)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)\n",
    "print(grid.best_estimator_.solver)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice.** Find the best score using gradient boosting tree model? Compare with team mates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search with cross validation\n",
    "\n",
    "Random search is an approach to parameter tuning that will sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations. A model is constructed and evaluated for each combination of parameters chosen. See: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the diabetes datasets\n",
    "dataset = datasets.load_diabetes()\n",
    "# alpha is sampled from a uniform distribution\n",
    "rand_params = {'alpha': uniform()}\n",
    "\n",
    "# create and fit a ridge regression model, testing random alpha values\n",
    "model = Ridge()\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=rand_params, n_iter=100, cv=5)\n",
    "rsearch.fit(dataset.data, dataset.target)\n",
    "\n",
    "# summarize the results of the random parameter search\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
