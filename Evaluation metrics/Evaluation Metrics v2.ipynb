{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "Ocha data science study group https://github.com/Fadouagh/data-ocha.git\n",
    "Author: Fadoua Ghourabi (fadouaghourabi@gmail.com)\n",
    "\n",
    "Date: July 4, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, R$^2$\n",
    "\n",
    "It is common sense to evaluate the performance of a machine learning model. So far in this study group, we used two evaluation metrics:\n",
    "- accuracy of a classification model is the fraction of correctly classified samples\n",
    "$$\\text{accuracy} = \\frac{\\text{number of correctly classified samples}}{\\text{total number of samples}}$$\n",
    "- score R$^2$ of a regression model is defined as follows\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{m}(y^{(i)} - \\hat{y^{(i)}})^2}{\\sum_{i=1}^{m}({y^{(i)}}-\\overline{y})^2},$$ where $m$ is the number of samples/observation, $y^{(i)}$ is the target value of sample $i$, $\\hat{y^{(i)}}$ is the predicted value and $\\overline{y}$ is the mean.\n",
    "\n",
    "However, accuracy and $R^2$ are not the only metrics. In some situations, they might be not appropriate. It is important **to select an evaluation metric that suits your application**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: critical application\n",
    "\n",
    "Suppose we are asked to make a classification model for early detection of cancer. If the result is negative \"no cancer\", the patient is assumed healthy. If the result is positive \"possibly cancer\", the patient receives additional diagnosis. Our model achieved high accuracy of 98%. Because the critical aspect of the application, we can't be satisfied with the 98% score. We need to ask ourselves what are the consequences of the model errors. If a healthy person is classified \"possibly cancer\", meaning an incorrect positive prediction or **false positive**, the person would go through expensive medical test (and possibly unnecessary distress). If a sick patient is classified \"no cancer\", meaning an incorrect negative prediction or **false negative**, serious health issues could be undetected. Besides the high accuracy, it is clear that the model should avoid false negative as much as possible.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: application with imbalanced dataset\n",
    "\n",
    "Suppose we you given a classification problem where one class is more frequent than the other. For instance, 99% of the data belongs to class A and 1% to class B. Even if we make a dummy classifier that returns class A all the time, we will get a high accuracy of 99%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n"
     ]
    }
   ],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.01669449081803"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(digits.target == 9)/digits.target.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: We make a model to classify the digits into \"nine\" and \"not nine\". First, we transform the data for binary classification. We change the target values 9 to True and the rest to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = digits.target == 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False,  True, False])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.9012620638455828\n",
      "Test score 0.8955555555555555\n"
     ]
    }
   ],
   "source": [
    "dummy_majority = DummyClassifier(strategy=\"most_frequent\").fit(X_train, y_train)\n",
    "print(\"Train score {}\".format(dummy_majority.score(X_train, y_train)))\n",
    "print(\"Test score {}\".format(dummy_majority.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.9383815887156645\n",
      "Test score 0.9177777777777778\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train)\n",
    "print(\"Train score {}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Test score {}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.9948032665181886\n",
      "Test score 0.9755555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"Train score {}\".format(log_reg.score(X_train, y_train)))\n",
    "print(\"Test score {}\".format(log_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.9955456570155902\n",
      "Test score 0.9955555555555555\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "print(\"Train score {}\".format(knn.score(X_train, y_train)))\n",
    "print(\"Test score {}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Which model among dummy classification, logistic regression and decision tree would you select for digit recognition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "A confusion matrix is a 2x2 array where rows are the true classes in $y$ and the columns are the predicted classes in $\\hat{y}$. Each cell corresponds to the number of samples. \n",
    "\n",
    "|           ${ }$    | predicted \"not nine\" | predicted \"nine\" |\n",
    "|          ---  |--- | --- |\n",
    "| true \"not nine\"| TN | FP |\n",
    "| true \"nine\"    | FN | TP |\n",
    "\n",
    "Where:\n",
    "- TN: true negative\n",
    "- TP: true positive \n",
    "- FN: false negative\n",
    "- FP: false positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dummy = dummy_majority.predict(X_test)\n",
    "pred_tree = tree.predict(X_test)\n",
    "pred_log_reg = log_reg.predict(X_test)\n",
    "pred_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dummy = confusion_matrix(y_test, pred_dummy)\n",
    "conf_tree = confusion_matrix(y_test, pred_tree)\n",
    "conf_log_reg = confusion_matrix(y_test, pred_log_reg)\n",
    "conf_knn = confusion_matrix(y_test, pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           ${ }$    | predicted \"not nine\" | predicted \"nine\" |\n",
    "|          ---  |--- | --- |\n",
    "| true \"not nine\"| TN | FP |\n",
    "| true \"nine\"    | FN | TP |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix of dummy classifier: \n",
      "[[403   0]\n",
      " [ 47   0]]\n",
      "Confusion matrix of decision tree classifier: \n",
      "[[390  13]\n",
      " [ 24  23]]\n",
      "Confusion matrix of logistic regression classifier: \n",
      "[[399   4]\n",
      " [  7  40]]\n",
      "Confusion matrix of KNN classifier: \n",
      "[[402   1]\n",
      " [  1  46]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix of dummy classifier: \\n{}\".format(conf_dummy))\n",
    "print(\"Confusion matrix of decision tree classifier: \\n{}\".format(conf_tree))\n",
    "print(\"Confusion matrix of logistic regression classifier: \\n{}\".format(conf_log_reg))\n",
    "print(\"Confusion matrix of KNN classifier: \\n{}\".format(conf_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the dummy classifier always predicts the most frequent class \"not nine\". The decision tree gives better results than the dummy classiffier. The logistic regression is slightly better than decision tree because it gives rise to fewer number of FP and FN. But the champion is KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Redefine the accuracy using TP, TN, FN and FP.\n",
    "Accuracy = $\\frac{\\text{number of correctly classified samples}}{\\text{total number of samples}} = \\frac{TN + TP}{TP + TN + FP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Precision measures how samples predicted as positive are actually positive.\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Precision is a metric that can be used in application where the number of false positives should be negligeable. Example: a bank uses a model to predict whether an applicant can afford a big loan. A False positive applicant could lead to complicated legal procedure for the bank and the applicant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of dummy classifier: \n",
      "0.0\n",
      "Precision of decision tree classifier: \n",
      "0.6388888888888888\n",
      "Precision of logistic regression classifier: \n",
      "0.9090909090909091\n",
      "Precision of KNN classifier: \n",
      "0.9787234042553191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "prec_dummy = precision_score(y_test, pred_dummy)\n",
    "prec_tree = precision_score(y_test, pred_tree)\n",
    "prec_log_reg = precision_score(y_test, pred_log_reg)\n",
    "prec_knn = precision_score(y_test, pred_knn)\n",
    "print(\"Precision of dummy classifier: \\n{}\".format(prec_dummy))\n",
    "print(\"Precision of decision tree classifier: \\n{}\".format(prec_tree))\n",
    "print(\"Precision of logistic regression classifier: \\n{}\".format(prec_log_reg))\n",
    "print(\"Precision of KNN classifier: \\n{}\".format(prec_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Recall measures the percentage of true positive samples are among positive predictions.\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "Recall is usefull when we want to limit the number of false negatives, for instance, the previous example of cancer diognosis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of dummy classifier: \n",
      "0.0\n",
      "Recall of decision tree classifier: \n",
      "0.48936170212765956\n",
      "Recall of logistic regression classifier: \n",
      "0.851063829787234\n",
      "Recall of KNN classifier: \n",
      "0.9787234042553191\n"
     ]
    }
   ],
   "source": [
    "recall_dummy = recall_score(y_test, pred_dummy)\n",
    "recall_tree = recall_score(y_test, pred_tree)\n",
    "recall_log_reg = recall_score(y_test, pred_log_reg)\n",
    "recall_knn = recall_score(y_test, pred_knn)\n",
    "print(\"Recall of dummy classifier: \\n{}\".format(recall_dummy))\n",
    "print(\"Recall of decision tree classifier: \\n{}\".format(recall_tree))\n",
    "print(\"Recall of logistic regression classifier: \\n{}\".format(recall_log_reg))\n",
    "print(\"Recall of KNN classifier: \\n{}\".format(recall_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f-score or f$_1$-score\n",
    "\n",
    "To summarize precision and recall, we can use another metric known as f-score.\n",
    "$$F = 2.\\frac{\\text{precision}\\times\\text{recall}}{\\text{precision}+\\text{recall}}$$\n",
    "\n",
    "f-score includes two metrics, precision and recall, in one number. However, it is harder to explain and interpret comparing to accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of dummy classifier: \n",
      "0.0\n",
      "Recall of decision tree classifier: \n",
      "0.5542168674698795\n",
      "Recall of logistic regression classifier: \n",
      "0.8791208791208791\n",
      "Recall of KNN classifier: \n",
      "0.9787234042553191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "f1_dummy = f1_score(y_test, pred_dummy)\n",
    "f1_tree = f1_score(y_test, pred_tree)\n",
    "f1_log_reg = f1_score(y_test, pred_log_reg)\n",
    "f1_knn = f1_score(y_test, pred_knn)\n",
    "print(\"Recall of dummy classifier: \\n{}\".format(f1_dummy))\n",
    "print(\"Recall of decision tree classifier: \\n{}\".format(f1_tree))\n",
    "print(\"Recall of logistic regression classifier: \\n{}\".format(f1_log_reg))\n",
    "print(\"Recall of KNN classifier: \\n{}\".format(f1_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can compute all three metrics for each class using function ``classification_report``. By the way, support is the number of occurence of each class in the target $y$. ``macro avg`` is the average of f-scores. ``weighted avg`` is the average of f-scores weighted by the support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.99      0.99       403\n",
      "        True       0.91      0.85      0.88        47\n",
      "\n",
      "    accuracy                           0.98       450\n",
      "   macro avg       0.95      0.92      0.93       450\n",
      "weighted avg       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
