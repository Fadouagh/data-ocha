{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models\n",
    "Author: Fadoua ghourabi (fadouaghourabi@gmail.com)\n",
    "\n",
    "Date: June 4, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms and notations\n",
    "\n",
    "Before we start, let's agree on some terms and notations.\n",
    "\n",
    "**Dataset** and **data** are both used interchangeably.\n",
    "\n",
    "A data has **features**, denoted $X$, and a **target**, denoted $y$. The values of $n$ features are denoted $x_i, \\ldots, x_n$. A data has $m > 0$ observations and the observation $j$ is denoted $x^j = [x_i^j, \\ldots, x_n^j]$ and $y^j$. The predicted target values are denoted $\\hat{y}$ and the mean of the target values is denoted $\\overline{y}$.\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "We use the following public datasets:\n",
    "\n",
    "- sklearn.datasets: scikit-learn comes with a few small toy datasets that do not require to download any file from  external website.\n",
    "    - load_boston() : boston house prices dataset\n",
    "    - load_iris(): Iris plants dataset\n",
    "    - load_diabetes(): diabetes dataset\n",
    "    - load_digits(): Optical recognition of handwritten digits dataset\n",
    "    - load_linnerud(): \n",
    "    - load_winse(): Wine recognition dataset\n",
    "    - load_breast(): Breast cancer diagnosis dataset\n",
    "\n",
    "- Real-world datasets downloaded from online databases. \n",
    "    - University of Standford datasets, e.g. http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv for advertising budget and sales in various markets\n",
    "    - openml.org\n",
    "    - others..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic concept of Linear regression\n",
    "\n",
    "Linear regression (or ordinary least squares) is the simplest model. The objective of a linear regression model is to find a relationship between one or more features and a **continuous** target variable. When there is only one feature it is called Univariate Linear Regression. If there are multiple features, it is called Multiple Linear Regression.\n",
    "\n",
    "<img src=\"linearRegression.png\" hight=300 width=400>\n",
    "\n",
    "Let $x_1$, $\\cdots$, $x_n$ be $n$ features and $y$ be the target. Linear regression model finds parameters $b$, $w_1$, $\\cdots$, $w_n$ such that:\n",
    "\n",
    "1. $\\hat{y} = \\sum^{n}_{i = 1} w_i x_i + b$, where $\\hat{y}$ is the predicted value, and\n",
    "2. the cost function $J(w,b)$ is minimum. \n",
    "\n",
    "The cost function is commonly used to assess the performance of a model. In the case of the linear regression model, the cost function is the **mean square error**: $$\\text{MSE} = J(w,b) = \\frac{1}{2m}\\sum^{m}_{i = 1}(y^i - \\hat{y}^i)^2$$, where $m$ is the number of observations.\n",
    "\n",
    "Now, let's code linear regression model from scratch! To that end, use data vectorization that we covered in numpy session.\n",
    "\n",
    "**Practice.** \n",
    "1. What vectors/matrices do we need? What are their shapes?\n",
    "2. Function ``linear_fct`` implements the vectorization $\\sum^{n}_{i = 1} w_i x_i + b$. Complete the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fct(X, W, b):\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Function ``MSE`` computes the cost function. Complete the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y,y_hat):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, X = np.array([[1.],[2.],[1.]]), 2., np.array([[1.,2.,1.],[3.,4.,3.2]])\n",
    "y = np.array([[6.9],[13.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = linear_fct(X, W, b)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Remember that linear regression finds parameters $b$, $w_1$, $\\cdots$, $w_n$ that minimize the cost function $J(w,b)$. We will use Gradient Descent to find the optimal values of the parameters.\n",
    "\n",
    "<img src=\"GD1.png\" hight=300 width=400>\n",
    "<img src=\"GD2.png\" hight=300 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure how the cost function changes when the parameters change. Therefore, we compute the partial derivatives of the cost function w.r.t to the parameters. $\\frac{\\delta{J}}{\\delta{b}}$ and $\\frac{\\delta{J}}{\\delta{w_i}}$, where $1\\leq i \\leq n$. \n",
    "\n",
    "**Practice.** \n",
    "1. Prove that $db = \\frac{\\delta{J}}{\\delta{b}} = \\frac{1}{m}\\sum^{m}_{j=1}(\\hat{y}^j - y^j)$ and $dw_i = \\frac{\\delta{J}}{\\delta{w_i}} = \\frac{1}{m}\\sum^{m}_{j=1}(\\hat{y}^j - y^j)x_{i}^j$?\n",
    "\n",
    "2. Complete function ``derivatibe`` that computes $db$ and $dw$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(X, y, y_hat):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    ### START CODE HERE ### (2 lines of code)\n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return db, dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, X = np.array([[1.],[2.],[1.]]), 2., np.array([[1.,2.,1.],[3.,4.,3.2]])\n",
    "y = np.array([[6.9],[13.8]])\n",
    "y_hat = linear_fct(X, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(X, y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting\n",
    "\n",
    "Fitting means computing parameters W and b that make the MSE minimum. When the function $J$ is concave, which is the case of the parabolic MSE function, we update the parameters towards to the global minumum of $J$. To that end, we update W and b as follows:\n",
    "\n",
    "$w_i = w_i - \\alpha \\frac{\\delta{J}}{\\delta{w_i}}$, and\n",
    "\n",
    "$b = b - \\alpha \\frac{\\delta{J}}{\\delta{b}}$\n",
    "\n",
    "$\\alpha$ is called the **learning rate**. W and b are updated recursively until the cost function converges to the minimum value. If the value of $\\alpha$ is too small, the cost function takes larger time to converge. If $\\alpha$ is too large, gradient descent may skip the minimum and fail to converge. $\\alpha$ is also called a **hyperparameter** of the regression model. Tuning hyperparameters is crucial when applying a machine learning model because it affects its performance.\n",
    "\n",
    "**Practice.** Function ``LR`` performs fitting. \n",
    "1. Complete the code of ``LR``.\n",
    "2. What is the role of parameter **tolerance**? **max_iterations**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR(X, y, alpha=0.01, tolerance=1.0e-5, max_iterations=1000000, desc=False):\n",
    "    \n",
    "    m = X.shape[0] # number of observations\n",
    "    n = X.shape[1] # number of features (independant variables)\n",
    "\n",
    "    # initialization of W and b\n",
    "    W = np.random.randn(n,1) # W is a (n,1) matrix of random numbers\n",
    "    b = 0 # b = 0\n",
    "    \n",
    "    mse_prev = 0 # initial value\n",
    "    i = 0 # number of iterations\n",
    "    while True:\n",
    "        i = i + 1\n",
    "        if (i > max_iterations):\n",
    "            print(\"\\033[31m Possible divergence of gradient descent.\\n Computation reached max iteration........will abort!\")\n",
    "            break\n",
    "            \n",
    "        ### START CODE HERE ### (2 lines of code)\n",
    "        # step 1. compute the predicted values\n",
    "\n",
    "        # step 2. compute the cost function\n",
    "\n",
    "        ### END CODE HERE ### \n",
    "        \n",
    "        if (abs(mse - mse_prev) < tolerance):\n",
    "            break\n",
    "            \n",
    "        # step 3. update the values of W and b \n",
    "        ### START CODE HERE ### (3 lines of code)\n",
    "       \n",
    "    \n",
    "    \n",
    "        ### END CODE HERE ### \n",
    "        \n",
    "        if (i % 100 == 0) & desc:\n",
    "            print(\"A iteration {0}, the cost function is {1}.\".format(i, mse))\n",
    "            \n",
    "        mse_prev = mse\n",
    "        \n",
    "    if desc:   \n",
    "        print(\"The total number of iterations is {0}\".format(i-1))  \n",
    "        print(\"Parameter W: {0}\".format(W))\n",
    "        print(\"Parameter b: {0}\".format(b))\n",
    "        print(\"Cost function: {0}\".format(mse))\n",
    "        \n",
    "    return b, W, mse, i-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, W, mse, i = LR(X, y, desc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we computed optimal values for b and W, we can perform predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    y_hat = np.dot(X,W) + b\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(X, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 110, 10\n",
    "X_new = np.random.randn(m,n)\n",
    "y_new = np.random.randn(m,1)\n",
    "W_new = np.random.randn(n,1)\n",
    "b_new = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_new[:,0], y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_new, b_new, mse, i = LR(X_new,y_new, desc=True) ## how many iterations you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the impact of $\\alpha$ on the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_alpha(X, y, alphas):\n",
    "    scores = []\n",
    "    for alpha in alphas:\n",
    "         b, W, mse, i = LR(X, y)\n",
    "         scores.append((alpha,mse,i))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [5,1,0.1,0.001,0.0001]\n",
    "test_alpha(X_new, y_new, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "Let's test our model on real-world data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = data.TV\n",
    "y0 = data.sales\n",
    "\n",
    "x1 = data.radio\n",
    "y1 = data.sales\n",
    "\n",
    "x2 = data.newspaper\n",
    "y2 = data.newspaper\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(131)\n",
    "plt.scatter(x0, y0, c='b', marker='o')\n",
    "plt.xlabel('TV')\n",
    "plt.ylabel('Sales')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(x1, y1, c='b', marker='o')\n",
    "plt.xlabel('Radio')\n",
    "plt.ylabel('Sales')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(x2, y2, c='b', marker='o')\n",
    "plt.xlabel('Newspaper')\n",
    "plt.ylabel('Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['newspaper'] #,'radio','newspaper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use the model we implemented above, we first must:\n",
    "1. convert the data to numpy arrays. This is a drawback of our model.\n",
    "2. make sure that the data has a valid shape. Recall: if n and m are the # of features and observations, respectively, then X is of shape (m, n), y and y_hat of shape (m, 1), W of shape (m, 1) and b a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_reshape(M):\n",
    "    \n",
    "    sM = M.shape\n",
    "    \n",
    "    if not isinstance(M, np.ndarray):\n",
    "        M = M.values\n",
    "    \n",
    "    if len(sM) == 1:\n",
    "        M = M.reshape((sM[0],1))\n",
    "    else:\n",
    "        M = M.reshape(sM)\n",
    "        \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = convert_and_reshape(X)\n",
    "y = convert_and_reshape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR(X,y,alpha=0.001,desc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['TV','radio','newspaper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = convert_and_reshape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR(X,y,alpha=0.0001,desc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model diverges when using three features... How can we revise the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[\"TV\",\"radio\",\"newspaper\"]]\n",
    "y = data.sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call ``train_test_split`` to split the data to train dataset and test dataset. Note that ``train_test_split`` works with DataFrame object. \n",
    "\n",
    "Other parameters:\n",
    "- ``stratify`` can be used when the target is a class. It means each target class should be represented with equal proportions. In our example, the two outcome classes, i.e. 0 and 1, have the same propositions in ``y_train`` and ``y_test``.\n",
    "- ``random_state=0`` makes sure that we obtain the same datasets everytime we run ``train_test_split``.\n",
    "- by default, the size of ``X_train`` and ``X_test`` are 75% and 25%, respectively. Use parameter ``test_size=0.33`` when working with ``X_test`` with size 33%, for instance.\n",
    "\n",
    "Result:\n",
    "- ``train_test_split`` returns two DataFrames ``X_train`` and ``X_test``, and two Series ``y_train`` and ``y_test``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model to the training datasets by calling ``LinearRegression().fit(X_train, y_train)``. We use the dataset ``X_test`` to predict the target values ``y_pred``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LinearRegression(verbose = 1)\n",
    "LR.fit(X_train, y_train)\n",
    "y_pred = LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters W and b are stored in the attributes ``coef_`` and ``intercept_``. The ``coef_`` attribute is a numpay array [$w_1$, $w_2$, $\\ldots$, $w_n$], where $w_i$ is the weight of feature $x_i$. The ``intercept_`` attribute is always a float number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The weights or coefs of the features are \", LR.coef_)\n",
    "print(\"The bias of the model is \", LR.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``LinearRegression()`` doesn't have a hyperparemeter $\\alpha$. It uses different algorithm to obtain the optimal values of the parameters. ``SGDRegressor()`` uses gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD = SGDRegressor(learning_rate=\"optimal\")\n",
    "SGD.fit(X_train, y_train)\n",
    "y_pred = SGD.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The weights or coefs of the features are \", LR.coef_)\n",
    "print(\"The bias of the model is \", LR.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation \n",
    "\n",
    "We compute the cost function to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE: \",mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure how well the model works by computing the accuracy, which is the fraction of sales that were correctly predicted. The accuracy is coefficient $$R^2 = 1 - \\frac{\\sum^{m}_{i=1}(y^i - \\hat{y}^i)^2}{\\sum^{m}_{i=1}(y^i - \\overline{y^i})^2}$$, where for an observation $i$, $y^i$ is the true values, $\\hat{y}^i$ is the predicted values and $\\overline{y}$ is the mean of all $y^i$. The close the score is to 1.0 the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation\n",
    "\n",
    "In cross validation, the data is split repeateadly and multiple data are trained. K-fold cross-validation is a common method, where 3 $\\leq$ K is a number specified by the user.\n",
    "\n",
    "<img src = \"crossvalidation.png\" height=\"200\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold.split(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_indices, test_indices in k_fold.split(X):      \n",
    "    print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ``cross_val_score`` returns the score of each test set in the k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(LR, X, y, cv=k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ``cross_val_predict`` has a similar interface to cross_val_score, but returns, **for each element in the input, the prediction that was obtained for that element when it was in the test set**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(LR, X, y, cv=k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y, predictions)\n",
    "plt.xlabel(\"y\")\n",
    "plt.ylabel(\"predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** What is the benifit of k-fold cross-validation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting vs. underfitting\n",
    "\n",
    "Overfitting occurs when a model is fit too closely to the training set. The model works well on the training set but not able to generalize to new data. In such case, the training score is high but the test score is low.\n",
    "\n",
    "Underfitting occurs when a model is too simple, e.g. \"Anyone who owns a house travels abroad.\". The model performs badly on both training dataset and test dataset.\n",
    "\n",
    "**Question.** Which of the follwoing models is overfitted? Underfitted?\n",
    "\n",
    "<img src=\"over_under.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned above, the score is a useful metric to decide whether a model is overfitted or underfitted. Other metrics are bias and variance.  The bias of a model is its average error for different training sets. The variance of a model indicates how sensitive it is to varying training sets. An overfitting model tends to have a high variance and a low bias. An underfitting model tends to have a high bias and a low variance.\n",
    "\n",
    "scikit-learn library recommends the following measures:\n",
    "\n",
    "**Case of high bias**\n",
    "- Add more features. \n",
    "- Use a more sophisticated model. \n",
    "- Use fewer samples. A high-bias algorithm can attain nearly the same error with a smaller training sample. For algorithms which are computationally expensive, reducing the training sample size can lead to very large improvements in speed.\n",
    "- Decrease regularization. Regularization is a technique used to impose simplicity in some machine learning models, by adding a penalty term that depends on the characteristics of the parameters. If a model has high bias, decreasing the effect of regularization can lead to better results.\n",
    "\n",
    "**Case of high variance**\n",
    "- Use fewer features. \n",
    "- Use more training samples. Adding training samples can reduce the effect of overfitting, and lead to improvements in a high variance estimator.\n",
    "- Increase Regularization. Regularization is designed to prevent overfitting. In a high-variance model, increasing regularization can lead to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "\n",
    "In ridge regression the coefficients $w_i$ are chosen to be as small as possible, i.e. with magnitudes close to 0. The cost function is defined as follows:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m}\\sum^{m}_{i = 1}(y^i - \\hat{y}^i)^2 + \\alpha \\sum^{n}_{i=1}(w_i)^2$$\n",
    "\n",
    "Ridge regression puts constraint on the coefficients, namely $\\alpha \\sum^{n}_{i=1}(w_i)^2 < c$ for some $c > 0$. The penalty term, $\\alpha$, regularizes the coefficients such that if the coefficients take large values the optimization function is penalized. So, ridge regression shrinks the coefficients and it helps to reduce the model complexity. Ridge regression uses **L2 regularization**. Obviously, when $\\alpha$ $\\to$ 0 , the cost function becomes similar to the linear regression cost function. So the lower the constraint (low $\\alpha$) on the features the closer the model to linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha = 0.1)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ridge = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ridge - mse # mse is the cost function of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficients with magnitude close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "y = pd.DataFrame(boston.target, columns = ['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "\n",
    "ridge05 = Ridge(alpha = 0.5)\n",
    "ridge05.fit(X_train, y_train)\n",
    "\n",
    "ridge1 = Ridge(alpha = 1)\n",
    "ridge1.fit(X_train, y_train)\n",
    "\n",
    "ridge10 = Ridge(alpha = 10)\n",
    "ridge10.fit(X_train, y_train)\n",
    "\n",
    "ridge100 = Ridge(alpha = 100)\n",
    "ridge100.fit(X_train, y_train)\n",
    "\n",
    "coef_index = [[1,2,3,4,5,6,7,8,9,10,11,12,13]]\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.plot(coef_index,LR.coef_, 'r*',linestyle='none')\n",
    "plt.title(r'Linear Regression ($\\alpha = 0$)')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.plot(coef_index,ridge05.coef_, 'b^',linestyle='none')\n",
    "plt.title(r'$\\alpha = 0.5$')\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.plot(coef_index,ridge1.coef_, 'mo',linestyle='none')\n",
    "plt.title(r'$\\alpha = 1$')\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.plot(coef_index,ridge10.coef_, 'gs',linestyle='none')\n",
    "plt.title(r'$\\alpha = 10$')\n",
    "\n",
    "plt.subplot(325)\n",
    "plt.plot(coef_index,ridge100.coef_, 'yd',linestyle='none')\n",
    "plt.title(r'$\\alpha = 100$')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "\n",
    "\n",
    "\n",
    "#plt.xlabel(\"Coefficient\")\n",
    "#plt.ylabel(\"Magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Discuss the above graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression vs. linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score (alpha = 0): \", LR.score(X_train, y_train))\n",
    "print(\"Test set score (alpha = 0): \", LR.score(X_test, y_test))\n",
    "\n",
    "print(\"Training set score (alpha = 0.5): \", ridge05.score(X_train, y_train))\n",
    "print(\"Test set score (alpha = 0.5): \", ridge05.score(X_test, y_test))\n",
    "\n",
    "print(\"Training set score (alpha = 1): \", ridge1.score(X_train, y_train))\n",
    "print(\"Test set score (alpha = 1): \", ridge1.score(X_test, y_test))\n",
    "\n",
    "print(\"Training set score (alpha = 10): \", ridge10.score(X_train, y_train))\n",
    "print(\"Test set score (alpha = 10): \", ridge10.score(X_test, y_test))\n",
    "\n",
    "print(\"Training set score (alpha = 100): \", ridge100.score(X_train, y_train))\n",
    "print(\"Test set score (alpha = 100): \", ridge100.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note from the book \"The elements of statistical learning. T. Hastie et. al.\"</h4><p>For a small subset of data < 400, ridge regression performs better than linear regression. No difference when samples are large enough.</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve, KFold\n",
    "\n",
    "def plot_scores(estimator, X, y, name):\n",
    "    training_set_size, train_scores, test_scores = learning_curve(estimator,X, y, train_sizes=np.linspace(.1, 1, 20), random_state=0, cv=KFold(n_splits=5)) \n",
    "    plt.plot(training_set_size,train_scores.mean(axis=1), \"r-\",label=\"training \" + name)\n",
    "    plt.plot(training_set_size,test_scores.mean(axis=1), \"r--\",label=\"test \" + name)\n",
    "    training_set_size, train_scores, test_scores = learning_curve(LinearRegression(),X, y, train_sizes=np.linspace(.1, 1, 20), random_state=0, cv=KFold(n_splits=5)) \n",
    "    plt.plot(training_set_size,train_scores.mean(axis=1), \"k-\",label=\"training LR\")\n",
    "    plt.plot(training_set_size,test_scores.mean(axis=1), \"k--\",label=\"test LR\")\n",
    "    plt.legend(loc=(1.5, 0), ncol=2, fontsize=11)\n",
    "    plt.ylim(0,1)\n",
    "    \n",
    "plt.subplot(311)    \n",
    "plot_scores(Ridge(alpha = 1), X, y, \"Ridge(alpha = 1)\")\n",
    "plt.subplot(312)    \n",
    "plot_scores(Ridge(alpha = 10), X, y, \"Ridge(alpha = 10)\")\n",
    "plt.subplot(313)    \n",
    "plot_scores(Ridge(alpha = 300), X, y, \"Ridge(alpha = 300)\")\n",
    "plt.subplots_adjust(wspace=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "- The training score is higher than the test score for ridge regression and linear regression.\n",
    "- The training score of ridge redgression is lower or equal than the one for linear regression. This is due to **regularization** or restricting the model to avoid overfitting.\n",
    "- The test score of ridge is better, in particular for small datasets < 400."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso \n",
    "\n",
    "Similar to ridge regression, lasso performs regularization. Lasso not only constraints the coefficient to be close to zero but also reduces some coefficients to zero. Features with coefficient equal to zero are therefore entirely ignored by the model. This can be useful for feature selection. The regularization used by lasso is called **L1 regularization**.\n",
    "\n",
    "The cost function of lasso is as follows:\n",
    "$$J(w,b) = \\frac{1}{2m}\\sum^{m}_{i = 1}(y^i - \\hat{y}^i)^2 + \\alpha \\sum^{n}_{i=1}|w_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lasso.coef_, 'r*',linestyle='none')\n",
    "plt.plot([0,0,0,0,0,0,0,0,0,0,0,0,0],'--',lw=0.5)\n",
    "plt.title('Lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.DESCR.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso is good at identifying useless features, such as festures with constant or almost constant values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['foo'] = 12.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['foo'] = 12.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lasso.coef_, 'r*',linestyle='none')\n",
    "plt.plot([0,0,0,0,0,0,0,0,0,0,0,0,0,0],'--',lw=0.5)\n",
    "plt.title('Lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost constant..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['foo'][:50] = 12.4\n",
    "X_train['foo'][50:200] = 9.06\n",
    "X_train['foo'][200:] = 10\n",
    "X_test['foo'][:14] = 10\n",
    "X_test['foo'][14:] = 12.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred = lasso.predict(X_test)\n",
    "plt.plot(lasso.coef_, 'r*',linestyle='none')\n",
    "plt.plot([0,0,0,0,0,0,0,0,0,0,0,0,0,0],'--',lw=0.5)\n",
    "plt.title('Lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to ridge regression, Lasso has a hypperparameter $\\alpha$ that controls the regularization. The default value of $\\alpha$ is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = 0.001)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression \n",
    "\n",
    "Binary logistic regression is a machine learning method used for prediction like any other regression analyses. Logistic regression is suitable for **classification problems** beacause it can be used to explain the relationship between features $x_1$, $\\ldots$, $x_n$ and a **class** $y$. For instance, whether features $x_1$, $\\ldots$, $x_n$ of a picture describe a cat or not. Estimation $\\hat{y}$ is regarded as a probability of a class.\n",
    "\n",
    "$$\\hat{y} = f(\\sum^{n}_{i = 1} w_i x_i + b),$$ where $f$ is called **activation function**.\n",
    "\n",
    "**Example: Binary logistic regression**\n",
    "<img src=\"LRegression.png\" width=400 hight=300>\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, i.e. $\\sigma(z) = \\frac{1}{1+e^{-z}}$ and $0 \\leq \\sigma(z)\\leq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogR = LogisticRegression(solver = 'newton-cg', multi_class='multinomial')\n",
    "LogR.fit(X_train, y_train)\n",
    "y_pred = LogR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogR.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogR.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
